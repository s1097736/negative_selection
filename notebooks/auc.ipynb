{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn.metrics\n",
    "\n",
    "def compute_matches(file_train, file_test, n, r):\n",
    "    # train using the given training file, and compute the scores for the given test file\n",
    "    cmd = f'java -jar negsel2.jar -self {file_train} -n {n} -r {r} -c -l < {file_test}'\n",
    "    p = os.popen(cmd)\n",
    "    return list(map(float, p.read().strip().split('\\n')))  # parse as list of floats\n",
    "\n",
    "\n",
    "def compute_scores(file_train, file_test_self, file_test_anomaly, n, r):\n",
    "    # compute the self and anomaly scores using the java program\n",
    "    scores_self = compute_matches(file_train, file_test_self, n, r)\n",
    "    scores_anomaly = compute_matches(file_train, file_test_anomaly, n, r)\n",
    "\n",
    "    # label the scores as self/anomaly, i.e. false/true\n",
    "    scores = np.array(\n",
    "        [[score, False] for score in scores_self] +\n",
    "        [[score,  True] for score in scores_anomaly]\n",
    "    )\n",
    "\n",
    "    # sort the scores\n",
    "    return scores[scores[:, 0].argsort()], len(scores_self), len(scores_anomaly)\n",
    "\n",
    "\n",
    "def compute_stats(scores, num_self, num_anomaly):\n",
    "    # compute all sensitivity and specificity values in O(n)\n",
    "    sensitivities = []\n",
    "    specificities = []\n",
    "\n",
    "    print(*scores, sep='\\n')\n",
    "\n",
    "    # Approach: compute sensitivity and specificity as normal, unless there are\n",
    "    # multiple entries sharing the same score next to each other. If there are,\n",
    "    # only compute the sensitivity and specificity after sweeping through to\n",
    "    # the largest index of that \"block\" of same-score datapoints. This\n",
    "    # reflects what happens in actual classification; a shift in the threshold\n",
    "    # affects the outcome for all datapoints with that score at once. \n",
    "\n",
    "    count_anomaly = 0\n",
    "    for cutoff_index in range(len(scores)):\n",
    "        score, anomalous = scores[cutoff_index]\n",
    "        \n",
    "        if cutoff_index < len(scores) -1:\n",
    "            next_score, _ = scores[cutoff_index + 1]\n",
    "        else:\n",
    "            next_score = -1\n",
    "\n",
    "        if anomalous:\n",
    "            count_anomaly += 1\n",
    "\n",
    "        # Checking if we're currently at the end of the \"block\" of same-score\n",
    "        # datapoints\n",
    "        if score != next_score:\n",
    "            sensitivities.append((num_anomaly - count_anomaly) / num_anomaly)\n",
    "            specificities.append(1 - (cutoff_index - count_anomaly) / num_self)\n",
    "\n",
    "\n",
    "    # add (0, 0) and (1, 1) and clip because of rounding errors\n",
    "    sensitivities = np.clip([1] + sensitivities + [0], 0, 1)\n",
    "    specificities = np.clip([1] + specificities + [0], 0, 1)\n",
    "\n",
    "    # compute auc\n",
    "    auc = sklearn.metrics.auc(specificities, sensitivities)\n",
    "    return sensitivities, specificities, auc\n",
    "\n",
    "\n",
    "def generate_plot(sensitivities, specificities, auc, file_train, file_test_self, file_test_anomaly, n, r):\n",
    "    plt.subplots(figsize=(8, 8))\n",
    "    plt.style.use('bmh')\n",
    "\n",
    "    plt.plot(specificities, sensitivities, color='orange')\n",
    "    plt.plot([0, 1], [0, 1], '--', color='#0e1111')\n",
    "\n",
    "    plt.xlabel('1 - specificity')\n",
    "    plt.ylabel('sensitivity')\n",
    "\n",
    "    plt.title(\n",
    "        f'Trained on {file_train}, Tested on {file_test_self} and {file_test_anomaly}\\n\\\n",
    "        Using n = {n} and r = {r}, AUC = {auc:.4f}'\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_9plot(results, file_train, file_test_self, file_test_anomaly, n):\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n",
    "\n",
    "    # hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "    for ax in axs.flat:\n",
    "        ax.label_outer()\n",
    "\n",
    "    # create the actual plots\n",
    "    for index, (sensitivities, specificities, auc) in enumerate(results):\n",
    "        axs[index // 3, index % 3].plot(specificities, sensitivities, color='orange')\n",
    "        axs[index // 3, index % 3].plot([0, 1], [0, 1], '--', color='#0e1111')\n",
    "        axs[index // 3, index % 3].set_title(f'r = {index + 1}, AUC = {auc:.4f}')\n",
    "\n",
    "    # give labels two big labels\n",
    "    fig.supxlabel('1 - specificity')\n",
    "    fig.supylabel('sensitivity')\n",
    "\n",
    "    # give on big title\n",
    "    fig.suptitle(\n",
    "        f'Trained on {file_train}, Tested on {file_test_self} and {file_test_anomaly}, n = {n}'\n",
    "    )\n",
    "    fig.tight_layout() # makes it less tight lol\n",
    "\n",
    "    plt.savefig(f'images/9plot_{file_train}_{file_test_self}_{file_test_anomaly}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_auc(file_train, file_test_self, file_test_anomaly, n, r):\n",
    "    scores, num_self, num_anomaly = compute_scores(file_train, file_test_self, file_test_anomaly, n, r)\n",
    "    sensitivities, specificities, auc = compute_stats(scores, num_self, num_anomaly)\n",
    "    generate_plot(sensitivities, specificities, auc, file_train, file_test_self, file_test_anomaly, n, r)\n",
    "\n",
    "\n",
    "def compute_aucs(file_train, file_test_self, file_test_anomaly, n):\n",
    "    results = []\n",
    "    for r in range(1, 10):\n",
    "        scores, num_self, num_anomaly = compute_scores(file_train, file_test_self, file_test_anomaly, n, r)\n",
    "        results.append(compute_stats(scores, num_self, num_anomaly))\n",
    "\n",
    "    generate_9plot(results, file_train, file_test_self, file_test_anomaly, n)\n",
    "\n",
    "\n",
    "def main():\n",
    "    file_train = 'english.train'\n",
    "    file_test_self = 'english.test'\n",
    "    file_test_anomaly = 'tagalog.test'\n",
    "\n",
    "    n = 10\n",
    "    r = 1\n",
    "\n",
    "    compute_auc(file_train, file_test_self, file_test_anomaly, n, r)\n",
    "    # compute_aucs(file_train, file_test_self, file_test_anomaly, n)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
